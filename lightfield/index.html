<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Programming Project - Image Mosaicing</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 1rem;
            text-align: center;
        }
        main {
            margin: 20px;
        }
        h2, h3 {
            color: #333;
        }
        .description, .procedure {
            margin-bottom: 20px;
        }
        .image-gallery {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }
        .image-item {
            width: calc(50% - 20px); /* 3 images per row */
            margin-bottom: 20px;
        }
        .image-item img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
        .image-item-full {
            width: 100%; /* Full-width image */
            margin-bottom: 20px;
        }
        .image-item-full img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
        .image-title {
            font-size: 1rem;
            text-align: center;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Lightfield Camera Project</h1>
        <h3>CS180: Intro to Computer Vision and Computational Photography</h3>
    </header>
    <main>
        <!-- Project Description -->
        <section class="description">
            <h2>Project Description</h2>
            <p>This project explores light field camera techniques by manipulating a grid of images taken along a single axis, demonstrating how shifting and averaging multiple images can simulate complex camera effects like depth refocusing and aperture adjustment. By capturing light rays from different perspectives, the project allows computational reconstruction of focus and aperture characteristics, revealing how simple mathematical operations can create sophisticated visual transformations that mimic advanced camera capabilities.</p>
        </section>

        <!-- Part 1: Shoot the Pictures -->
        <section class="procedure">
            <h2>Depth Refocusing</h2>
            <p>Depth refocusing is a computational photography technique that exploits the parallax effect, where light rays from different points in a scene arrive at slightly different angles when the camera moves. By capturing a grid of images along a single axis, the technique captures how light rays from objects at different distances shift relative to each other, creating a unique depth-dependent positioning effect. The method centers on a reference image and shifts surrounding images using a scaling factor (alpha), which mathematically adjusts the trajectories of these light rays to simulate different focal planes. As nearby objects cause more significant light ray displacement compared to distant objects, the shifting and averaging process creates images that can selectively sharpen or blur different depth regions. By using alpha values ranging from -1 to 3, the technique manipulates these light ray intersections, effectively allowing computational reconstruction of focus at various depths without physically changing the camera's lens.</p>
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Focus on far-depth, achieved with alpha=-0.50</div>
                    <img src="media/-0.50.png" alt="Left Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Right Image</div>
                    <img src="media/2.50.png" alt="Right Image">
                </div>
            </div>
        </section>

        <!-- Part 2: Recover Homographies -->
        <section class="procedure">
            <h2>Recover Homographies</h2>
            <section class="procedure">
                <h2>Homography Process</h2>
                <p>A <strong>homography</strong> is a transformation between two images that describes how points in one image correspond to points in another, based on a projective transformation. The homography matrix <em>H</em> is a 3x3 matrix that maps a point <em>p</em> in one image to its corresponding point <em>p'</em> in another image, using the equation:</p>
                <pre>
                    p' = H * p
                </pre>
            
                <p>Here, <em>p</em> and <em>p'</em> are points in homogeneous coordinates, which means they are represented as 3D vectors <code>[x, y, 1]</code>. The homography matrix <em>H</em> has eight degrees of freedom and is typically expressed as:</p>
                <pre>
                    H = | h11  h12  h13 |
                        | h21  h22  h23 |
                        | h31  h32  h33 |
                </pre>
            
                <p>To compute <em>H</em>, we need at least 4 pairs of corresponding points between two images. Each point correspondence gives two equations, and with four point pairs, we have enough equations to solve for the 8 unknowns (the ninth value is set to 1 for normalization). The system of equations is solved using methods like Singular Value Decomposition (SVD).</p>
            
                <p>Once the homography matrix is computed, we can apply it to map every pixel in one image to its corresponding location in the second image, enabling us to warp one image into the perspective of the other.</p>
            </section>

            <div class="image-item">
                    <div class="image-title">Left Image Correspondence Points</div>
                    <img src="media/left_points.png" alt="Left Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Right Image</div>
                    <img src="media/right_points.png" alt="Right Image">
                </div>
        </section>

        <!-- Part 3: Warp the Images -->
        <section class="procedure">
            <h2>Image Warping Process</h2>
            <p>The warping process aligns one image to the perspective of another using a <strong>homography matrix</strong>, which defines how points in one image map to corresponding points in another. The homography is a 3x3 matrix that accounts for transformations such as translation, rotation, scaling, and perspective changes.</p>
            
            <p>To apply the homography, each pixel in the source image is transformed to new coordinates in the target image. This is known as <strong>forward warping</strong>. In this project, the left image is warped to match the right image by calculating where each pixel from the left image should appear in the final mosaic.</p>
        
            <p>A <strong>translation matrix</strong> is also applied to ensure that all parts of the warped image fit within the output canvas, preventing any negative or out-of-bounds pixel coordinates. The warped image is then placed into this larger canvas, aligning it with the reference image.</p>
        
            <p>The result is a transformed version of the left image, reprojected onto the same plane as the right image, ready for blending into a mosaic.</p>
        </section>


        <!-- Part 5: Blend the Images into a Mosaic -->
        <section class="procedure">
            <h2>Blending and Mosaicing Process</h2>
            <p>In this project, images are blended into a mosaic using <strong>weighted averaging</strong> to ensure smooth transitions in overlapping regions. First, the left image is warped into the perspective of the right image using a homography matrix. To blend them, we create <strong>alpha masks</strong> for both images, indicating valid pixels. These masks are then used to compute <strong>blend masks</strong> that determine the contribution of each image in the overlapping areas.</p>
        
            <p>The <strong>weighted averaging</strong> formula for each image is:</p>
            <pre>
                blend_mask1 = mask1 / (mask1 + mask2 + ϵ), 
                blend_mask2 = mask2 / (mask1 + mask2 + ϵ)
            </pre>
        
            <p>This ensures that pixels are mixed smoothly based on their mask values. In the final step, the pixel values from both images are combined using these blend masks, resulting in a seamless mosaic where edges between the images are blurred and integrated naturally. This avoids harsh transitions and creates a coherent panoramic view.</p>
            <div class="image-item-full">
                <div class="Final Mosaic">Right Image</div>
                <img src="media/final.png" alt="final">
            </div>
        </section>


        <!-- Part 4: Image Rectification -->
        <section class="procedure">
            <h2>Image Rectification</h2>
            <p>Image rectification is the process of transforming an image so that a certain planar surface appears front-facing and rectangular. We achieve this by computing a homography based on known correspondences.
            In this case, we take an image of a building from a perspective. Then I create correspondence points for the 4 corners of the right plane of this building. We will transform this into a front facing, rectangular surface
            and visualize the final result of the image when this is done. In this case, I transformed this into size 100,200 as this seems to be the approximate ratio of that face in the original image.
            You might notice that the final result does not have a perfect rectangle. This is because it is very sensitive to the correspondence points not representing the perfect corners of the rectangle
                in the original building. I was not able to get
            perfect points because there is noise in selecting points on an image manually.</p>
            <p>Below is an example of image rectification:</p>
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Original Image</div>
                    <img src="media/rectify.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Rectified Image</div>
                    <img src="media/Warped_building.png" alt="Rectified Image">
                </div>
            </div>
        </section>


        <section class="procedure">
            <h2>Feature Matching for Autostitching</h2>
            <p>Using the harris interest point detector, corners of interest are selected based on directional gradients and thresholded for the final
                corner set.</p>
            <div class="image-gallery">
                <div class="image-item-full">
                    <div class="image-title">Harris Corners</div>
                    <img src="media/harris.png" alt="Original Image">
                </div>
            </div>
        </section>

        <section class="procedure">
            <h2>Adaptive Non-Maximal Suppression</h2>
            <p>Because there are too many Harris corners to efficiently compute, we use adaptive non maximal suppression to eliminate some corners.
           This approach involves iterating over each corner and identifying the nearest corner that has a significantly larger value, defined
                by the condition \( x_i < c_{\text{robustness}} \times x_j \), where \( x_i \) is the current corner, \( x_j \)
               represents all other corners, and \( c_{\text{robustness}} \) is set to 0.9. Next, the corners with the greatest distance
               to another corner meeting this threshold are selected. I set the number of selected corners to 500.</p>
            <div class="image-gallery">
                <div class="image-item-full">
                    <div class="image-title">ANMS corners</div>
                    <img src="media/anms.png" alt="Original Image">
                </div>
            </div>
        </section>

        <section class="procedure">
            <h2>Feature Matching</h2>
            <p>To filter matching corners, a feature matching technique is applied. For each corner, a 40x40 patch is extracted from the image, centered on the corner point. This patch is resized to 8x8, normalized by subtracting the mean and dividing by the standard deviation to minimize intensity differences, and then flattened into a 1D feature vector. For each feature in image A, its error is calculated against all features in image B. Using Lowe’s ratio test, if the ratio of the two smallest errors is below a threshold of 0.5, the feature in image B with the smallest error is selected as the best match for the feature in image A.</p>
            <div class="image-gallery">
                <div class="image-item-full">
                    <div class="image-title"> 1/3 of the Feature Matches</div>
                    <img src="media/matches_balcony.png" alt="Original Image">
                </div>
            </div>
        </section>

        <section class="procedure">
            <h2>RANSAC</h2>
            <p>
                Unfortunately, some matches are still not very accurate as correspondence points. The reason I say not very accurate is because they cause
                large shifts in calculations of H, the homography matrix. RANSAC is an iterative process that tests homography matrices from subsets
                of points of all the matches by warping 4 points using the homography matrix, and then labeling those points who show accurate results
                as inliers. The threshold of how close warped points have to be to their computed match is a parameter, which I set to 5 pixels. 
            </p>
            <div class="image-gallery">
                <div class="image-item-full">
                    <div class="image-title">Some RANSAC inliers</div>
                    <img src="media/inliers.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Manual bedroom blend</div>
                    <img src="media/final.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Auto-stitching bedroom blend</div>
                    <img src="media/bedroom_blend.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Manual balcony view blend</div>
                    <img src="media/manual_balcony.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Auto-stitching balcony view blend</div>
                    <img src="media/balcony_blend.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Manual corner blend</div>
                    <img src="media/manual_corner.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Auto-stitching corner blend</div>
                    <img src="media/corner_blend.png" alt="Original Image">
                </div>
            </div>
            <p>
                It seems that there are mixed results. For the balcony, you can see less blur in the orange area for the auto stitched mosaic,
                indicating better results. For the corner image, you can see a better computed homography as the images are more properly aligned.
                However, for the bedroom, it seems that the manual is actually better.
                The reason for this and also what was really my cool takeaway from this project is that it makes perfect sense that doing this process
                manually can actually lead to better results. Both of these methods are simply forms of picking corresponding points - the matching
                algorithms are the same. Since I spent a lot of time picking many correspondence points for the bedroom image manually, it actually 
                turns out that this has a better result. 
                However, it is without a doubt that the RANSAC auto stitching method using harris corners is superior when generalized to more complex
                situations. The efficiency to compute so many correspondence points is unmatched. Although using gradients may be less effective on
                an indivdual point scale, generalizing to thousands or millions of points requires something like harris plus RANSAC, since it is more efficient,
                and likely very accurate because the sample size is now huge and noise will often balance out. 
            </p>
        </section>
    </main>
</body>
</html>
