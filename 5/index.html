<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Programming Project - Image Mosaicing</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 1rem;
            text-align: center;
        }
        main {
            margin: 20px;
        }
        h2, h3 {
            color: #333;
        }
        .description, .procedure {
            margin-bottom: 20px;
        }
        .image-gallery {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }
        .image-item {
            width: calc(50% - 20px); /* 3 images per row */
            margin-bottom: 20px;
        }
        .image-item img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
        .image-item-full {
            width: 100%; /* Full-width image */
            margin-bottom: 20px;
        }
        .image-item-full img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
        .image-title {
            font-size: 1rem;
            text-align: center;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Project 5: Diffusion Models</h1>
        <h3>CS180: Intro to Computer Vision and Computational Photography</h3>
    </header>
    <main>
        <!-- Project Description -->
        <section class="description">
            <h2>Project Description</h2>
            <p>In this project, we play around with diffusion models.</p>
        </section>

        <!-- Part 0-->
        <section class="procedure">
            <h2>Sample Captions and Created Images</h2>
            <p>Here are a few model outputs for 3 text prompts. The first set of 3 photos is for 5 inference steps.
              For the first two photos, the text prompt seems to be reflected somewhat well. Although the images do
              still seem a bit noisy and not perfectly resembling of the prompt. In fact for the last one, we see that the image does seem
            to reflect the texture of a rocket ship, but it doesn't exactly resemble a rocket ship. It's clear that more 
              inference steps are necessary for a better output. </p>
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Inference Steps = 5</div>
                    <img src="media/5steps.png" alt="Left Image">
                </div>
            </div>
            <p> As you can see, with more inference steps, we get much better and clearer output that
            resembles the text prompt much better.</p>
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Inference Steps = 15</div>
                    <img src="media/15steps.png" alt="Left Image">
                </div>
            </div>
        </section>

        <!-- Part 1: Sampling Loops -->
        <section class="procedure">
           <h2>Forward Process</h2>
           <p>
             The forward process involves taking a clean image and adding noise to it. Given a clean image x_0, we get
             a noisy image x_t by sampling from a Gaussian with mean (sqrt(alphabar_t)) * x_0 and variance 1-alphabar_t.
           </p>

            <div class="image-item">
                  <div class="image-title">Campanile at different noise levels.</div>
                  <img src="media/1_1.png" alt="Left Image">
            </div>
        </section>

        <!-- Part 1.2: Classical Denoising -->
        <section class="procedure">
            <h2>Denoising with Gaussian Blur Filtering</h2>
            <p>Here we show the results of denoising these images using Gaussian blur filtering to remove the noise at 
            the same timesteps of noise (250, 500, 750) that we displayed noisy images for in the last part.</p>
          <div class="image-item">
                  <div class="image-title">Side by side Gaussian blur filtering</div>
                  <img src="media/1_2.png" alt="Left Image">
            </div>
        </section>
      


        <!-- Part 1.3: One-Step Denoising -->
        <section class="procedure">
            <h2>One Step Denoising</h2>
            <p>
              In this part, we use a pretrained diffusion model to recover Gaussian noise from an image
              and then remove that noise to denoise the image. This is essentially one step denoising because it is 
              not iterative, and we are simply passing in the timestep of noise into the model so it can remove all the noise
              conditioned on that timestep at once. 
            </p>
            <div class="image-item-full">
                <div class="Final Mosaic">One Step Denoised Campanile at Different Timestamps</div>
                <img src="media/1_3.png" alt="final">
            </div>
        </section>


        <!-- Part 4: Iterative Denoising -->
        <section class="procedure">
            <h2>Iterative Denoising</h2>
            <p>
                I implemented iterative denoising for a diffusion model using **strided timesteps** to skip redundant steps, improving efficiency while preserving image quality. Starting with a noisy image at a high timestep, I used a U-Net to predict the noise and estimate the clean image, iteratively refining it across progressively less noisy timesteps. The denoising process interpolates between the noisy input and the predicted clean signal using diffusion-specific parameters like \( \alpha_t \), \( \beta_t \), and \( \bar{\alpha}_t \). By skipping steps with strided timesteps, the model efficiently reconstructs high-quality images without the need for full-timestep computations. I visualized the gradual denoising process and compared the results to single-step denoising and Gaussian blurring, demonstrating the superiority of the iterative approach. This project highlights the power of diffusion models in balancing computational efficiency and high-quality image restoration.
            </p>
              <div class="image-item-full">
                  <div class="image-title">Results</div>
                  <img src="media/1_4.png" alt="Original Image">
              </div>

        </section>

        <!-- Part 5: Diffusion Model Sampling -->
        <section class="procedure">
            <h2>Diffusion Model Sampling</h2>
            <p>If we repeat part 1.4, starting from random noise and timestep = 0, we can effectively generate images from scratch
            or "sample" this diffusion model. Here are 5 results for the text prompt "a high quality photo".</p>
            <div class="image-item-full">
                <div class="image-title">Diffusion Model Samples</div>
                <img src="media/1_5.png" alt="Original Image">
            </div>

        </section>

        <!-- Part 6: CFG (Classifier Free Guidance) -->
        <section class="procedure">
            <h2>CFG (Classifier Free Guidance)</h2>
            <p>
                In this part, I implemented **Classifier-Free Guidance (CFG)** to improve the quality of generated images by combining conditional and unconditional noise estimates. The denoising process involves calculating a weighted combination of these estimates, controlled by a guidance scale \( \gamma \), where higher values amplify the conditional guidance for better image fidelity. I created a function, `iterative_denoise_cfg`, that iteratively refines noisy inputs using both a meaningful text prompt and an empty ("null") prompt to guide the generation process. Using this function, I generated five images with a CFG scale of \( \gamma = 7 \), which balances conditional alignment and diversity. This approach produces sharper, more detailed images compared to standard diffusion methods, demonstrating the effectiveness of classifier-free guidance.
            </p>

            <div class="image-item">
                <div class="image-title">CFG generated images</div>
                <img src="media/1_6.png" alt="Original Image">
            </div>

        </section>

        <!-- Part 7: Image-to-image Translation -->
        <section class="procedure">
            <h2>Image to Image translation</h2>
            <p>
                In this part, I used **SDEdit** to edit images by adding varying levels of noise and then iteratively denoising them using **Classifier-Free Guidance (CFG)**. For each noise level (\(1, 3, 5, 7, 10, 20\)), I added noise to the original image to push it away from the image manifold and used a diffusion model with the prompt "a high quality photo" to guide it back. The denoising process forced the noisy images to align with the natural image manifold, gradually restoring details while preserving edits induced by the noise. I repeated this procedure for three test images, producing a range of outputs for each noise level, showing progressively refined versions as the noise level decreased. This approach demonstrates the modelâ€™s ability to creatively edit and restore images while balancing between the original content and new generative adjustments.
            </p>            
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Campanile</div>
                    <img src="media/1_7_camp.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Steph</div>
                    <img src="media/1_7_curry.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Lambo</div>
                    <img src="media/1_7_lambo.png" alt="Original Image">
                </div>
            </div>
        </section>

        <section class="procedure">
            <h2>RANSAC</h2>
            <p>
                Unfortunately, some matches are still not very accurate as correspondence points. The reason I say not very accurate is because they cause
                large shifts in calculations of H, the homography matrix. RANSAC is an iterative process that tests homography matrices from subsets
                of points of all the matches by warping 4 points using the homography matrix, and then labeling those points who show accurate results
                as inliers. The threshold of how close warped points have to be to their computed match is a parameter, which I set to 5 pixels. 
            </p>
            <div class="image-gallery">
                <div class="image-item-full">
                    <div class="image-title">Some RANSAC inliers</div>
                    <img src="media/inliers.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Manual bedroom blend</div>
                    <img src="media/final.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Auto-stitching bedroom blend</div>
                    <img src="media/bedroom_blend.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Manual balcony view blend</div>
                    <img src="media/manual_balcony.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Auto-stitching balcony view blend</div>
                    <img src="media/balcony_blend.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Manual corner blend</div>
                    <img src="media/manual_corner.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Auto-stitching corner blend</div>
                    <img src="media/corner_blend.png" alt="Original Image">
                </div>
            </div>
            <p>
                It seems that there are mixed results. For the balcony, you can see less blur in the orange area for the auto stitched mosaic,
                indicating better results. For the corner image, you can see a better computed homography as the images are more properly aligned.
                However, for the bedroom, it seems that the manual is actually better.
                The reason for this and also what was really my cool takeaway from this project is that it makes perfect sense that doing this process
                manually can actually lead to better results. Both of these methods are simply forms of picking corresponding points - the matching
                algorithms are the same. Since I spent a lot of time picking many correspondence points for the bedroom image manually, it actually 
                turns out that this has a better result. 
                However, it is without a doubt that the RANSAC auto stitching method using harris corners is superior when generalized to more complex
                situations. The efficiency to compute so many correspondence points is unmatched. Although using gradients may be less effective on
                an indivdual point scale, generalizing to thousands or millions of points requires something like harris plus RANSAC, since it is more efficient,
                and likely very accurate because the sample size is now huge and noise will often balance out. 
            </p>
        </section>
    </main>
</body>
</html>

