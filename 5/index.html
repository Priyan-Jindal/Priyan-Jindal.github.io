<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Programming Project - Image Mosaicing</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 1rem;
            text-align: center;
        }
        main {
            margin: 20px;
        }
        h2, h3 {
            color: #333;
        }
        .description, .procedure {
            margin-bottom: 20px;
        }
        .image-gallery {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }
        .image-item {
            width: calc(50% - 20px); /* 3 images per row */
            margin-bottom: 20px;
        }
        .image-item img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
        .image-item-full {
            width: 100%; /* Full-width image */
            margin-bottom: 20px;
        }
        .image-item-full img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
        }
        .image-title {
            font-size: 1rem;
            text-align: center;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Project 5: Diffusion Models</h1>
        <h3>CS180: Intro to Computer Vision and Computational Photography</h3>
    </header>
    <main>
        <!-- Project Description -->
        <section class="description">
            <h2>Project Description</h2>
            <p>In this project, we play around with diffusion models.</p>
        </section>

        <!-- Part 0-->
        <section class="procedure">
            <h2>Sample Captions and Created Images</h2>
            <p>Here are a few model outputs for 3 text prompts. The first set of 3 photos is for 5 inference steps.
              For the first two photos, the text prompt seems to be reflected somewhat well. Although the images do
              still seem a bit noisy and not perfectly resembling of the prompt. In fact for the last one, we see that the image does seem
            to reflect the texture of a rocket ship, but it doesn't exactly resemble a rocket ship. It's clear that more 
              inference steps are necessary for a better output. </p>
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Inference Steps = 5</div>
                    <img src="media/5steps.png" alt="Left Image">
                </div>
            </div>
            <p> As you can see, with more inference steps, we get much better and clearer output that
            resembles the text prompt much better.</p>
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Inference Steps = 15</div>
                    <img src="media/15steps.png" alt="Left Image">
                </div>
            </div>
        </section>

        <!-- Part 1: Sampling Loops -->
        <section class="procedure">
           <h2>Forward Process</h2>
           <p>
             The forward process involves taking a clean image and adding noise to it. Given a clean image x_0, we get
             a noisy image x_t by sampling from a Gaussian with mean (sqrt(alphabar_t)) * x_0 and variance 1-alphabar_t.
           </p>

            <div class="image-item">
                  <div class="image-title">Campanile at different noise levels.</div>
                  <img src="media/1_1.png" alt="Left Image">
            </div>
        </section>

        <!-- Part 1.2: Classical Denoising -->
        <section class="procedure">
            <h2>Denoising with Gaussian Blur Filtering</h2>
            <p>Here we show the results of denoising these images using Gaussian blur filtering to remove the noise at 
            the same timesteps of noise (250, 500, 750) that we displayed noisy images for in the last part.</p>
          <div class="image-item">
                  <div class="image-title">Side by side Gaussian blur filtering</div>
                  <img src="media/1_2.png" alt="Left Image">
            </div>
        </section>
      


        <!-- Part 1.3: One-Step Denoising -->
        <section class="procedure">
            <h2>One Step Denoising</h2>
            <p>
              In this part, we use a pretrained diffusion model to recover Gaussian noise from an image
              and then remove that noise to denoise the image. This is essentially one step denoising because it is 
              not iterative, and we are simply passing in the timestep of noise into the model so it can remove all the noise
              conditioned on that timestep at once. 
            </p>
            <div class="image-item-full">
                <div class="Final Mosaic">One Step Denoised Campanile at Different Timestamps</div>
                <img src="media/1_3.png" alt="final">
            </div>
        </section>


        <!-- Part 4: Iterative Denoising -->
        <section class="procedure">
            <h2>Iterative Denoising</h2>
            <p>
                I implemented iterative denoising for a diffusion model using **strided timesteps** to skip redundant steps, improving efficiency while preserving image quality. Starting with a noisy image at a high timestep, I used a U-Net to predict the noise and estimate the clean image, iteratively refining it across progressively less noisy timesteps. The denoising process interpolates between the noisy input and the predicted clean signal using diffusion-specific parameters like \( \alpha_t \), \( \beta_t \), and \( \bar{\alpha}_t \). By skipping steps with strided timesteps, the model efficiently reconstructs high-quality images without the need for full-timestep computations. I visualized the gradual denoising process and compared the results to single-step denoising and Gaussian blurring, demonstrating the superiority of the iterative approach. This project highlights the power of diffusion models in balancing computational efficiency and high-quality image restoration.
            </p>
              <div class="image-item-full">
                  <div class="image-title">Results</div>
                  <img src="media/1_4.png" alt="Original Image">
              </div>

        </section>

        <!-- Part 5: Diffusion Model Sampling -->
        <section class="procedure">
            <h2>Diffusion Model Sampling</h2>
            <p>If we repeat part 1.4, starting from random noise and timestep = 0, we can effectively generate images from scratch
            or "sample" this diffusion model. Here are 5 results for the text prompt "a high quality photo".</p>
            <div class="image-item-full">
                <div class="image-title">Diffusion Model Samples</div>
                <img src="media/1_5.png" alt="Original Image">
            </div>

        </section>

        <!-- Part 6: CFG (Classifier Free Guidance) -->
        <section class="procedure">
            <h2>CFG (Classifier Free Guidance)</h2>
            <p>
                In this part, I implemented **Classifier-Free Guidance (CFG)** to improve the quality of generated images by combining conditional and unconditional noise estimates. The denoising process involves calculating a weighted combination of these estimates, controlled by a guidance scale \( \gamma \), where higher values amplify the conditional guidance for better image fidelity. I created a function, `iterative_denoise_cfg`, that iteratively refines noisy inputs using both a meaningful text prompt and an empty ("null") prompt to guide the generation process. Using this function, I generated five images with a CFG scale of \( \gamma = 7 \), which balances conditional alignment and diversity. This approach produces sharper, more detailed images compared to standard diffusion methods, demonstrating the effectiveness of classifier-free guidance.
            </p>

            <div class="image-item">
                <div class="image-title">CFG generated images</div>
                <img src="media/1_6.png" alt="Original Image">
            </div>

        </section>

        <!-- Part 7: Image-to-image Translation -->
        <section class="procedure">
            <h2>Image to Image translation</h2>
            <p>
                In this part, I used **SDEdit** to edit images by adding varying levels of noise and then iteratively denoising them using **Classifier-Free Guidance (CFG)**. For each noise level (\(1, 3, 5, 7, 10, 20\)), I added noise to the original image to push it away from the image manifold and used a diffusion model with the prompt "a high quality photo" to guide it back. The denoising process forced the noisy images to align with the natural image manifold, gradually restoring details while preserving edits induced by the noise. I repeated this procedure for three test images, producing a range of outputs for each noise level, showing progressively refined versions as the noise level decreased. This approach demonstrates the model’s ability to creatively edit and restore images while balancing between the original content and new generative adjustments.
            </p>            
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Campanile</div>
                    <img src="media/1_7_camp.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Steph</div>
                    <img src="media/1_7_curry.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Lambo</div>
                    <img src="media/1_7_lambo.png" alt="Original Image">
                </div>
            </div>
        </section>

        <!-- Part 7.1: Image-to-image Translation -->
        <section class="procedure">
            <h2>Editing Hand-Drawn Images and Web Images</h2>
            <p>
I experimented with projecting non-realistic images onto the natural image manifold using iterative denoising with Classifier-Free Guidance. I edited a web-sourced image of a car and two hand-drawn images—a sports car and a pair of sunglasses—at various noise levels, transforming them into photorealistic versions.            </p>            
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Web Image - Car</div>
                    <img src="media/1_7_1_car.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Hand Drawn - Sports Car</div>
                    <img src="media/1_7_1_spcar.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Hand Drawn - Sunglasses</div>
                    <img src="media/1_7_1_glasses.png" alt="Original Image">
                </div>
            </div>
        </section>

        <!-- Part 7.2: Image-to-image Translation -->
        <section class="procedure">
            <h2>Inpainting</h2>
            <p>
I implemented an inpainting function based on the RePaint paper to reconstruct missing regions in images using a binary mask. By iteratively denoising the input image, I replaced the masked regions with new content generated by the diffusion model while keeping the unmasked areas unchanged. I applied this technique to inpaint the top of the Campanile and experimented with two additional custom images and masks for creative edits.            
            <div class="image-gallery">
                <div class="image-item">
                    <div class="image-title">Campanile Inpainted</div>
                    <img src="media/1_7_2.png" alt="Original Image">
                </div>
            </div>
        </section>

        <!-- Part 7.3: Image-to-image Translation -->
        <section class="procedure">
            <h2>Text-Conditional Image to Image Translation</h2>
            <p>
In this task, I extended the SDEdit approach to include text prompts, enabling guided transformations of images based on specific descriptions. By varying noise levels (\([1, 3, 5, 7, 10, 20]\)), I projected test images onto the natural image manifold while aligning them with a provided text prompt, such as "a rocket ship." I applied this method to the test image and two additional custom images, creating results that blend the original structure with the semantic guidance of the text prompt.            <div class="image-gallery">
            <div class="image-gallery">            
                <div class="image-item">
                    <div class="image-title">Campanile with Rocket prompt</div>
                    <img src="media/1_7_3_camp.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Steph Curry with Old Man Prompt</div>
                    <img src="media/1_7_3_curry.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Lambo with Man Wearing a Hat prompt</div>
                    <img src="media/1_7_3_lambo.png" alt="Original Image">
                </div>
            </div>
        </section>

        <!-- Part 8: Image-to-image Translation -->
        <section class="procedure">
            <h2>Visual Anagrams</h2>
            <p>
This code implements a **visual anagram creation** process where an image changes its semantic content depending on its orientation, based on prompts like "an oil painting of an old man" and "an oil painting of people around a campfire." The key idea is to iteratively denoise an image with **Classifier-Free Guidance (CFG)** for two different text prompts: one for the original image orientation and one for its flipped version. The denoising is guided by conditional and unconditional noise estimates, combined with a scaling factor to control the strength of the guidance.

For each timestep \(t\), the code computes the conditional noise estimates for both prompts using the U-Net. It also calculates the unconditional noise estimates by passing the same images through the model with an empty prompt. These estimates are combined using CFG to calculate the guided noise estimates, where:
\[
\text{noise\_est\_cfg} = \text{uncond\_noise\_est} + \text{scale} \cdot (\text{cond\_noise\_est} - \text{uncond\_noise\_est}).
\]

For the flipped version of the image, the process is repeated after flipping the image vertically, and the resulting noise estimate is flipped back to align with the original image orientation. The final noise estimate is then averaged across the two prompts (original and flipped) to harmonize the two semantic interpretations. 

The resulting noise estimate is used to compute the clean image estimate for the current timestep using the equation:
\[
\text{clean\_est} = \frac{\text{image} - \text{noise\_est\_final} \cdot \sqrt{1 - \alpha_t}}{\sqrt{\alpha_t}}.
\]

The predicted image for the previous timestep is computed by interpolating between the clean image estimate and the current noisy image, with variance correction added using the predicted variance from the U-Net. This iterative denoising process ensures that the final image contains visual elements corresponding to both prompts, depending on the orientation of the image. The code successfully demonstrates how to generate creative visual anagrams by leveraging the powerful capabilities of diffusion models.            
            <div class="image-gallery">            
                <div class="image-item">
                    <div class="image-title">An oil painting of an old man</div>
                    <img src="media/1_8_oldman.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">An oil painting of people around a campfire</div>
                    <img src="media/1_8_youngman.png" alt="Original Image">
                </div>
            </div>
            <div class="image-gallery">            
                <div class="image-item">
                    <div class="image-title">An oil painting of a snowy mountain village</div>
                    <img src="media/1_8_snow_amalfi.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">A photo of the amalfi coast</div>
                    <img src="media/1_8_amalfi.png" alt="Original Image">
                </div>
            </div>
            <div class="image-gallery">            
                <div class="image-item">
                    <div class="image-title">A lithograph of waterfalls</div>
                    <img src="media/1_8_litho.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">A lithograph of a skull</div>
                    <img src="media/1_8_lithowards.png" alt="Original Image">
                </div>
            </div>
        </section>
<!-- Part 9: Image-to-image Translation -->
        <section class="procedure">
            <h2>Visual Anagrams</h2>
            <p>
                This code implements **Factorized Diffusion** to create hybrid images that combine low and high-frequency components from two separate noise estimates generated using different prompts. Specifically, it creates an image that appears as a "lithograph of a skull" when viewed from a distance but resembles "a lithograph of waterfalls" when viewed up close. The process involves iteratively denoising the image while separating the frequency components of two noise estimates using Gaussian blurring.

For each timestep \(t\), the code calculates the **conditional noise estimates** (\(\epsilon_{1}, \epsilon_{2}\)) for the two prompts and their respective **unconditional noise estimates** using the U-Net. These estimates are combined using **Classifier-Free Guidance (CFG)**, where the final guided noise for each prompt is:
\[
\text{noise\_est} = \text{uncond\_noise} + \text{scale} \cdot (\text{cond\_noise} - \text{uncond\_noise}).
\]
The low-frequency component is extracted by applying a Gaussian blur (\(kernel\_size=33, sigma=2\)) to the first noise estimate. The high-frequency component is derived by subtracting the blurred version of the second noise estimate from itself. These components are combined to form the final noise estimate:
\[
\text{noise\_est\_final} = \text{low\_pass} + \text{high\_pass}.
\]

The denoised image for the current timestep is computed using:
\[
\text{clean\_est} = \frac{\text{image} - \text{noise\_est\_final} \cdot \sqrt{1 - \alpha_t}}{\sqrt{\alpha_t}},
\]
where \(\alpha_t\) and \(\beta_t = 1 - \alpha_t\) are timestep-dependent noise parameters. The predicted image for the previous timestep is calculated by interpolating between the clean estimate and the noisy image, while incorporating the predicted variance to stabilize the results.

This process is repeated iteratively across the timesteps, gradually refining the image to combine the high and low frequencies of the prompts. The result is a visually compelling hybrid image that seamlessly integrates the features of the two concepts, leveraging the capabilities of diffusion models for creative compositional outputs.
            
            </p>
            <div class="image-gallery">            
                <div class="image-item">
                    <div class="image-title">Hybrid image of a skull and a waterfall</div>
                    <img src="media/1_9_skull.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">Hybrid image of a barista and a dog</div>
                    <img src="media/1_9_dog.png" alt="Original Image">
                </div>
                <div class="image-item">
                    <div class="image-title">A photo of a rocket and a pencil</div>
                    <img src="media/1_9_rocket.png" alt="Original Image">
                </div>
            </div>
                
    </main>
</body>
</html>

